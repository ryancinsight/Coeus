//! WordPiece Tokenizer Example
//!
//! This example demonstrates the production-ready WordPiece tokenizer implementation
//! showcasing elite programming practices and design principles.

use rustllm_core::{
    core::{
        plugin::TokenizerPlugin,
        tokenizer::{Token, Tokenizer},
        traits::{foundation::Named, identity::Versioned},
    },
    foundation::error::Result,
};
use rustllm_tokenizer_wordpiece::WordPieceTokenizerPlugin;
use std::borrow::Cow;

fn main() -> Result<()> {
    println!("üî§ WordPiece Tokenizer Example");
    println!("==============================");
    
    // Create the WordPiece tokenizer plugin
    let plugin = WordPieceTokenizerPlugin::default();
    println!("‚úì Created WordPiece tokenizer plugin");
    println!("   ‚Ä¢ Name: {}", plugin.name());
    println!("   ‚Ä¢ Version: {}", plugin.version());
    
    // Create tokenizer instance
    let tokenizer = plugin.create_tokenizer()?;
    println!("‚úì Created tokenizer instance");
    println!("   ‚Ä¢ Name: {}", tokenizer.name());
    println!("   ‚Ä¢ Vocabulary Size: {}", tokenizer.vocab_size());
    
    // Test texts with different complexity levels
    let test_texts = vec![
        ("Simple", "hello world"),
        ("Complex", "The quick brown fox jumps over the lazy dog"),
        ("Technical", "WordPiece tokenization algorithm implementation"),
        ("Unicode", "caf√© na√Øve r√©sum√©"),
        ("Mixed", "Hello, ‰∏ñÁïå! How are you today?"),
    ];
    
    println!("\nüß™ Tokenization Examples");
    println!("=========================");
    
    for (category, text) in test_texts {
        println!("\nüìù {} Text: \"{}\"", category, text);
        
        // Tokenize the text
        let start_time = std::time::Instant::now();
        let tokens: Vec<_> = tokenizer.tokenize(Cow::Borrowed(text)).collect();
        let tokenize_time = start_time.elapsed();
        
        println!("   üî§ Tokens ({}):", tokens.len());
        for (i, token) in tokens.iter().enumerate() {
            if let Some(token_str) = token.as_str() {
                println!("      {}. \"{}\" (ID: {})", i + 1, token_str, token.id());
            }
        }
        
        // Decode back to text
        let start_time = std::time::Instant::now();
        let decoded = tokenizer.decode(tokens.clone())?;
        let decode_time = start_time.elapsed();
        
        println!("   üìÑ Decoded: \"{}\"", decoded);
        println!("   ‚è±Ô∏è  Timing:");
        println!("      ‚Ä¢ Tokenization: {:.2}Œºs", tokenize_time.as_micros());
        println!("      ‚Ä¢ Decoding: {:.2}Œºs", decode_time.as_micros());
        
        // Calculate compression ratio
        let original_chars = text.chars().count();
        let token_count = tokens.len();
        let compression_ratio = token_count as f64 / original_chars as f64;
        println!("   üìä Stats:");
        println!("      ‚Ä¢ Characters: {}", original_chars);
        println!("      ‚Ä¢ Tokens: {}", token_count);
        println!("      ‚Ä¢ Compression Ratio: {:.2}", compression_ratio);
    }
    
    // Demonstrate trie-based vocabulary lookup performance
    println!("\n‚ö° Performance Demonstration");
    println!("============================");
    
    let large_text = "The WordPiece tokenization algorithm is a subword tokenization technique that was introduced by Google for their BERT model. It works by starting with a vocabulary of individual characters and iteratively merging the most frequent pairs of adjacent symbols to create new subword units. This approach allows the model to handle out-of-vocabulary words by breaking them down into smaller, known subword pieces.";
    
    println!("üìù Large Text ({} characters):", large_text.len());
    println!("   \"{}...\"", &large_text[..100]);
    
    // Benchmark tokenization
    let iterations = 100usize;
    let start_time = std::time::Instant::now();

    for _ in 0..iterations {
        let _tokens: Vec<_> = tokenizer.tokenize(Cow::Borrowed(large_text)).collect();
    }

    let total_time = start_time.elapsed();
    let avg_time = total_time / iterations as u32;
    let chars_per_second = (large_text.len() * iterations) as f64 / total_time.as_secs_f64();
    
    println!("‚è±Ô∏è  Benchmark Results ({} iterations):", iterations);
    println!("   ‚Ä¢ Total Time: {:.2}ms", total_time.as_millis());
    println!("   ‚Ä¢ Average Time: {:.2}Œºs", avg_time.as_micros());
    println!("   ‚Ä¢ Throughput: {:.0} chars/second", chars_per_second);
    
    // Demonstrate algorithm features
    println!("\nüèóÔ∏è  Algorithm Features");
    println!("=======================");
    println!("‚úì Trie-based vocabulary lookup: O(k) where k is token length");
    println!("‚úì Unicode normalization: NFD normalization for consistency");
    println!("‚úì Greedy longest-match: Optimal subword segmentation");
    println!("‚úì Zero-copy processing: Iterator-based with minimal allocations");
    println!("‚úì Special token handling: [CLS], [SEP], [UNK], [PAD], [MASK]");
    println!("‚úì Subword continuation: ## prefix for word pieces");
    
    // Design principles demonstrated
    println!("\nüéØ Design Principles Demonstrated");
    println!("==================================");
    println!("‚úì SOLID: Single responsibility, interface segregation");
    println!("‚úì CUPID: Composable, Unix philosophy, predictable, idiomatic");
    println!("‚úì Zero-Cost Abstractions: Efficient iterator-based processing");
    println!("‚úì Memory Efficiency: Trie-based vocabulary and COW strings");
    println!("‚úì Type Safety: Compile-time guarantees and bounds checking");
    println!("‚úì Mathematical Foundations: Proper Unicode handling and normalization");
    
    println!("\nüéâ WordPiece tokenizer demonstration completed successfully!");
    
    Ok(())
}
