//! Enhanced Transformer Model Example
//!
//! This example demonstrates the production-ready transformer implementation
//! showcasing elite programming practices and design principles.

use rustllm_core::{
    core::{
        model::{ForwardModel, Model, ModelBuilder, Transformer250MConfig},
        plugin::ModelBuilderPlugin,
    },
    foundation::error::Result,
};
use rustllm_model_transformer::TransformerModelPlugin;

fn main() -> Result<()> {
    println!("ğŸš€ Enhanced Transformer Model Example");
    println!("=====================================");

    // Create the enhanced transformer plugin directly
    let transformer_plugin = TransformerModelPlugin::default();
    println!("âœ“ Created enhanced transformer plugin");

    // Create model configuration
    let config = Transformer250MConfig {
        hidden_dim: 512,
        num_layers: 8,
        num_heads: 8,
        vocab_size: 32000,
        max_seq_len: 1024,
        intermediate_dim: 2048,
        layer_norm_eps: 1e-5,
        use_bias: true,
        dropout: 0.1,
        attention_dropout: 0.1,
        activation: "gelu".to_string(),
    };

    println!("ğŸ“‹ Model Configuration:");
    println!("   â€¢ Hidden Dimension: {}", config.hidden_dim);
    println!("   â€¢ Number of Layers: {}", config.num_layers);
    println!("   â€¢ Number of Heads: {}", config.num_heads);
    println!("   â€¢ Vocabulary Size: {}", config.vocab_size);
    println!("   â€¢ Max Sequence Length: {}", config.max_seq_len);
    println!("   â€¢ Intermediate Dimension: {}", config.intermediate_dim);

    // Validate configuration
    config.validate()?;
    println!("âœ“ Configuration validated");

    // Calculate parameter count
    let param_count = config.num_parameters();
    println!(
        "ğŸ“Š Total Parameters: {} ({:.1}M)",
        param_count,
        param_count as f64 / 1_000_000.0
    );

    // Create model builder directly from plugin
    let builder = transformer_plugin.create_builder()?;

    println!("âœ“ Created model builder");

    // Build the enhanced transformer model
    println!("ğŸ”¨ Building enhanced transformer model...");
    let model = builder.build(config)?;

    println!("âœ“ Model built successfully!");
    println!("ğŸ“ˆ Model Statistics:");
    println!("   â€¢ Parameters: {}", model.num_parameters());
    println!(
        "   â€¢ Memory Usage: ~{:.1} MB",
        model.num_parameters() as f64 * 4.0 / 1_000_000.0
    );

    // Test forward pass with sample input
    println!("\nğŸ§ª Testing Forward Pass");
    println!("========================");

    let sample_tokens = vec![1, 15, 42, 128, 256, 512, 1024, 2048];
    println!("ğŸ“ Input tokens: {:?}", sample_tokens);

    let start_time = std::time::Instant::now();
    let output = model.forward(sample_tokens.clone())?;
    let inference_time = start_time.elapsed();

    println!(
        "âœ“ Forward pass completed in {:.2}ms",
        inference_time.as_millis()
    );
    println!("ğŸ“Š Output shape: [{}] (vocabulary logits)", output.len());

    // Find top-k predictions
    let mut indexed_logits: Vec<(usize, f32)> = output
        .iter()
        .enumerate()
        .map(|(i, &logit)| (i, logit))
        .collect();
    indexed_logits.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap());

    println!("ğŸ¯ Top-5 Predictions:");
    for (rank, (token_id, logit)) in indexed_logits.iter().take(5).enumerate() {
        println!("   {}. Token {}: {:.4}", rank + 1, token_id, logit);
    }

    // Performance metrics
    let tokens_per_second = sample_tokens.len() as f64 / inference_time.as_secs_f64();
    println!("\nâš¡ Performance Metrics:");
    println!("   â€¢ Inference Time: {:.2}ms", inference_time.as_millis());
    println!("   â€¢ Tokens/Second: {:.1}", tokens_per_second);
    println!(
        "   â€¢ Parameters/Second: {:.1}M",
        model.num_parameters() as f64 / inference_time.as_secs_f64() / 1_000_000.0
    );

    // Architecture highlights
    println!("\nğŸ—ï¸  Architecture Highlights");
    println!("============================");
    println!(
        "âœ“ Multi-Head Attention with {} heads",
        model.config().num_heads
    );
    println!("âœ“ Sinusoidal Positional Encoding");
    println!("âœ“ GELU Activation Functions");
    println!("âœ“ Layer Normalization (Pre-norm)");
    println!("âœ“ Residual Connections");
    println!("âœ“ Causal Masking for Autoregressive Generation");

    // Design principles demonstrated
    println!("\nğŸ¯ Design Principles Demonstrated");
    println!("==================================");
    println!("âœ“ SOLID: Single responsibility, open/closed, interface segregation");
    println!("âœ“ CUPID: Composable, Unix philosophy, predictable, idiomatic");
    println!("âœ“ Zero-Cost Abstractions: Efficient iterator-based processing");
    println!("âœ“ Memory Efficiency: Arena allocators and parameter layout");
    println!("âœ“ Type Safety: Compile-time guarantees and bounds checking");
    println!("âœ“ Mathematical Foundations: Numerically stable implementations");

    println!("\nğŸ‰ Enhanced transformer demonstration completed successfully!");

    Ok(())
}
